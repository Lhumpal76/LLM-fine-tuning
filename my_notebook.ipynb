{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from datasets import Dataset, DatasetDict\n",
        "import torch\n",
        "import os\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "\n",
        "from huggingface_hub import login\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "\n",
        "# Install bitsandbytes\n",
        "!pip install bitsandbytes\n",
        "\n",
        "# Get Hugging Face token from Kaggle secrets\n",
        "user_secrets = UserSecretsClient()\n",
        "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "\n",
        "# Log in to Hugging Face\n",
        "login(token=hf_token)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model and paths\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "model_path = \"/kaggle/working/base_model\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
        "    print(\"Loaded model from local directory...\")\n",
        "except:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "    model.save_pretrained(model_path)\n",
        "    tokenizer.save_pretrained(model_path)\n",
        "    print(\"Downloaded and saved model...\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load and process dataset\n",
        "data_dir = \"/kaggle/input/hunting-beast-youtube-transcripts\"\n",
        "all_text = \"\"\n",
        "for file in os.listdir(data_dir):\n",
        "    file_path = os.path.join(data_dir, file)\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        all_text += f.read() + \" \"\n",
        "\n",
        "chunk_size = 2048\n",
        "text_chunks = [{\"text\": all_text[i:i + chunk_size]} for i in range(0, len(all_text), chunk_size)]\n",
        "full_dataset = Dataset.from_list(text_chunks)\n",
        "\n",
        "split_ratio = 0.8\n",
        "train_size = int(split_ratio * len(full_dataset))\n",
        "train_dataset = full_dataset.select(range(train_size))\n",
        "test_dataset = full_dataset.select(range(train_size, len(full_dataset)))\n",
        "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "\n",
        "# GPU monitoring thread\n",
        "def monitor_gpu():\n",
        "    while True:\n",
        "        try:\n",
        "            output = subprocess.check_output(['nvidia-smi'], stderr=subprocess.STDOUT)\n",
        "            print(output.decode('utf-8'))\n",
        "            if torch.cuda.is_available():\n",
        "                print(f\"PyTorch CUDA memory allocated: {torch.cuda.memory_allocated() / (1024**3):.2f} GB\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error executing nvidia-smi: {e.output.decode('utf-8')}\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"nvidia-smi not found. Make sure NVIDIA drivers are installed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "        time.sleep(5)\n",
        "\n",
        "gpu_monitor_thread = threading.Thread(target=monitor_gpu, daemon=True)\n",
        "gpu_monitor_thread.start()\n",
        "\n",
        "# Fine-tuning function\n",
        "def fine_tune_LLM(model):\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=2048, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    tokenized_data = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "    model.train()\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"fine_tuned_model\",\n",
        "        gradient_checkpointing=True,\n",
        "        gradient_checkpointing_kwargs={'use_reentrant': False},\n",
        "        overwrite_output_dir=True,\n",
        "        learning_rate=1e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=1,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"logs\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        fp16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=tokenized_data[\"train\"],\n",
        "        eval_dataset=tokenized_data[\"test\"],\n",
        "        args=training_args,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    print(\"\\nBeginning to train model...\")\n",
        "    trainer.train()\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "    return eval_results[\"eval_loss\"]\n",
        "\n",
        "# Run fine-tuning\n",
        "eval_loss = fine_tune_LLM(model)\n",
        "print(f\"Final Evaluation Loss: {eval_loss}\")"
      ],
      "metadata": {
        "id": "KE_VnbQdFHTo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}